{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Manifold-Constrained Hyper-Connections (mHC)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bassrehab/mhc-visualizer/blob/main/notebook/mhc_exploration.ipynb)\n",
    "\n",
    "This notebook demonstrates the key insight from DeepSeek's mHC paper:\n",
    "**unconstrained residual mixing matrices cause signal explosion in deep networks,\n",
    "while doubly stochastic constraints keep signals bounded.**\n",
    "\n",
    "We'll implement Sinkhorn-Knopp projection and visualize why it matters.\n",
    "\n",
    "**Paper:** https://arxiv.org/abs/2512.24880\n",
    "\n",
    "**Author:** Subhadip Mitra (contact@subhadipmitra.com)\n",
    "\n",
    "**Repository:** https://github.com/bassrehab/mhc-visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running in Colab)\n",
    "# !pip install -q numpy matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sinkhorn-Knopp Algorithm\n",
    "\n",
    "The Sinkhorn-Knopp algorithm projects any positive matrix onto the set of **doubly stochastic matrices** - matrices where all rows and columns sum to 1.\n",
    "\n",
    "### Why Doubly Stochastic?\n",
    "\n",
    "Doubly stochastic matrices have a crucial property: **they are closed under multiplication**. This means:\n",
    "- If A and B are doubly stochastic, then A @ B is also doubly stochastic\n",
    "- The spectral norm is bounded: ||A|| ≤ 1\n",
    "- Signal propagation stays bounded even through many layers\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "Starting from any positive matrix, we alternate between:\n",
    "1. Normalizing rows to sum to 1\n",
    "2. Normalizing columns to sum to 1\n",
    "\n",
    "This converges to a doubly stochastic matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp(matrix: np.ndarray, iterations: int = 20, epsilon: float = 1e-8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Project a matrix onto the set of doubly stochastic matrices\n",
    "    using the Sinkhorn-Knopp algorithm.\n",
    "    \n",
    "    Args:\n",
    "        matrix: Input matrix (will be exponentiated to ensure positivity)\n",
    "        iterations: Number of Sinkhorn iterations\n",
    "        epsilon: Small value for numerical stability\n",
    "        \n",
    "    Returns:\n",
    "        Doubly stochastic matrix (rows and columns sum to 1)\n",
    "    \"\"\"\n",
    "    # Exponentiate to ensure all entries are positive\n",
    "    P = np.exp(matrix)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Normalize rows\n",
    "        P = P / (P.sum(axis=1, keepdims=True) + epsilon)\n",
    "        # Normalize columns\n",
    "        P = P / (P.sum(axis=0, keepdims=True) + epsilon)\n",
    "    \n",
    "    return P\n",
    "\n",
    "\n",
    "def is_doubly_stochastic(matrix: np.ndarray, tol: float = 1e-6) -> Tuple[bool, float, float]:\n",
    "    \"\"\"\n",
    "    Check if a matrix is doubly stochastic.\n",
    "    \n",
    "    Returns:\n",
    "        (is_valid, row_error, col_error)\n",
    "    \"\"\"\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    col_sums = matrix.sum(axis=0)\n",
    "    \n",
    "    row_error = np.max(np.abs(row_sums - 1))\n",
    "    col_error = np.max(np.abs(col_sums - 1))\n",
    "    \n",
    "    is_valid = row_error < tol and col_error < tol\n",
    "    return is_valid, row_error, col_error\n",
    "\n",
    "\n",
    "print(\"Sinkhorn-Knopp implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Demo: Single Matrix Projection\n",
    "\n",
    "Let's see how a random matrix becomes doubly stochastic through Sinkhorn projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random matrix\n",
    "n = 4\n",
    "random_matrix = np.random.randn(n, n)\n",
    "\n",
    "# Project onto doubly stochastic\n",
    "projected = sinkhorn_knopp(random_matrix, iterations=20)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original matrix\n",
    "im1 = axes[0].imshow(random_matrix, cmap='RdBu', vmin=-2, vmax=2)\n",
    "axes[0].set_title('Original Random Matrix')\n",
    "axes[0].set_xlabel(f'Row sums: {random_matrix.sum(axis=1).round(2)}')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Projected matrix\n",
    "im2 = axes[1].imshow(projected, cmap='Blues', vmin=0, vmax=0.5)\n",
    "axes[1].set_title('After Sinkhorn Projection (20 iters)')\n",
    "axes[1].set_xlabel(f'Row sums: {projected.sum(axis=1).round(3)}')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify doubly stochastic property\n",
    "is_valid, row_err, col_err = is_doubly_stochastic(projected)\n",
    "print(f\"\\nIs doubly stochastic: {is_valid}\")\n",
    "print(f\"Max row sum deviation: {row_err:.2e}\")\n",
    "print(f\"Max col sum deviation: {col_err:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Manifold Dial: Varying Sinkhorn Iterations\n",
    "\n",
    "This is the key insight! Watch how the matrix transforms as we increase iterations:\n",
    "- **0 iterations**: Just exp(M), unconstrained\n",
    "- **2-5 iterations**: Partial projection, some structure\n",
    "- **10-20 iterations**: Fully doubly stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"Manifold Dial\" - sweep through Sinkhorn iterations\n",
    "iterations_to_test = [0, 1, 2, 5, 10, 20]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Use same random matrix for all\n",
    "base_matrix = np.random.randn(4, 4)\n",
    "\n",
    "for idx, iters in enumerate(iterations_to_test):\n",
    "    if iters == 0:\n",
    "        # Just exponentiate, no normalization\n",
    "        result = np.exp(base_matrix)\n",
    "    else:\n",
    "        result = sinkhorn_knopp(base_matrix, iterations=iters)\n",
    "    \n",
    "    # Check properties\n",
    "    is_valid, row_err, col_err = is_doubly_stochastic(result)\n",
    "    max_val = result.max()\n",
    "    \n",
    "    im = axes[idx].imshow(result, cmap='Blues', vmin=0, vmax=max(0.5, result.max()))\n",
    "    axes[idx].set_title(f'{iters} iterations')\n",
    "    axes[idx].set_xlabel(f'Row err: {row_err:.2e}')\n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "plt.suptitle('The Manifold Dial: Sinkhorn Iterations Transform Random → Doubly Stochastic', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability Metrics\n",
    "\n",
    "To understand why mHC is stable, we need to measure how matrices affect signal propagation:\n",
    "\n",
    "- **Forward Gain**: Maximum row sum (how much a signal can be amplified)\n",
    "- **Backward Gain**: Maximum column sum (gradient flow)\n",
    "- **Spectral Norm**: Largest singular value (operator norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_gain(matrix: np.ndarray) -> float:\n",
    "    \"\"\"Maximum row sum - measures worst-case signal amplification.\"\"\"\n",
    "    return np.max(np.sum(np.abs(matrix), axis=1))\n",
    "\n",
    "\n",
    "def backward_gain(matrix: np.ndarray) -> float:\n",
    "    \"\"\"Maximum column sum - measures worst-case gradient amplification.\"\"\"\n",
    "    return np.max(np.sum(np.abs(matrix), axis=0))\n",
    "\n",
    "\n",
    "def spectral_norm(matrix: np.ndarray) -> float:\n",
    "    \"\"\"Largest singular value - the operator norm.\"\"\"\n",
    "    return np.linalg.svd(matrix, compute_uv=False)[0]\n",
    "\n",
    "\n",
    "# Compare metrics for HC vs mHC\n",
    "print(\"Comparing a single random matrix (HC) vs Sinkhorn-projected (mHC):\\n\")\n",
    "\n",
    "# HC: Random matrix (exponentiated for fair comparison)\n",
    "hc_matrix = np.exp(np.random.randn(4, 4))\n",
    "\n",
    "# mHC: Sinkhorn-projected\n",
    "mhc_matrix = sinkhorn_knopp(np.random.randn(4, 4), iterations=20)\n",
    "\n",
    "print(\"HC (unconstrained):\")\n",
    "print(f\"  Forward gain:  {forward_gain(hc_matrix):.4f}\")\n",
    "print(f\"  Backward gain: {backward_gain(hc_matrix):.4f}\")\n",
    "print(f\"  Spectral norm: {spectral_norm(hc_matrix):.4f}\")\n",
    "\n",
    "print(\"\\nmHC (doubly stochastic):\")\n",
    "print(f\"  Forward gain:  {forward_gain(mhc_matrix):.4f}\")\n",
    "print(f\"  Backward gain: {backward_gain(mhc_matrix):.4f}\")\n",
    "print(f\"  Spectral norm: {spectral_norm(mhc_matrix):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Explosion Problem: Composite Mapping\n",
    "\n",
    "The real issue isn't single-layer behavior—it's what happens when we **compose many layers**.\n",
    "\n",
    "In a deep network, the effective transformation is:\n",
    "$$H_{composite} = H_L \\cdot H_{L-1} \\cdot ... \\cdot H_1$$\n",
    "\n",
    "Let's simulate this for both HC and mHC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_residual_matrix(n: int, method: str, sinkhorn_iters: int = 20) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a residual mixing matrix.\n",
    "    \n",
    "    Args:\n",
    "        n: Matrix size\n",
    "        method: 'baseline' (identity), 'hc' (random), or 'mhc' (Sinkhorn-projected)\n",
    "        sinkhorn_iters: Iterations for mHC\n",
    "    \"\"\"\n",
    "    if method == 'baseline':\n",
    "        return np.eye(n)\n",
    "    elif method == 'hc':\n",
    "        # Random matrix - this is the problem!\n",
    "        return np.random.randn(n, n)\n",
    "    elif method == 'mhc':\n",
    "        # Sinkhorn-projected - this is the solution!\n",
    "        return sinkhorn_knopp(np.random.randn(n, n), iterations=sinkhorn_iters)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "\n",
    "def simulate_depth(depth: int, n: int, method: str, sinkhorn_iters: int = 20) -> List[float]:\n",
    "    \"\"\"\n",
    "    Simulate composite forward gain over depth.\n",
    "    \n",
    "    Returns list of forward gains at each layer.\n",
    "    \"\"\"\n",
    "    composite = np.eye(n)\n",
    "    gains = []\n",
    "    \n",
    "    for _ in range(depth):\n",
    "        H = generate_residual_matrix(n, method, sinkhorn_iters)\n",
    "        composite = H @ composite\n",
    "        gains.append(forward_gain(composite))\n",
    "    \n",
    "    return gains\n",
    "\n",
    "\n",
    "print(\"Simulation functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate composite mapping explosion\n",
    "depth = 64\n",
    "n = 4\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "baseline_gains = simulate_depth(depth, n, 'baseline')\n",
    "\n",
    "np.random.seed(42)\n",
    "hc_gains = simulate_depth(depth, n, 'hc')\n",
    "\n",
    "np.random.seed(42)\n",
    "mhc_gains = simulate_depth(depth, n, 'mhc', sinkhorn_iters=20)\n",
    "\n",
    "# Plot!\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.semilogy(baseline_gains, 'g-', linewidth=2, label='Baseline (Identity)', alpha=0.8)\n",
    "plt.semilogy(hc_gains, 'r-', linewidth=2, label='HC (Random)', alpha=0.8)\n",
    "plt.semilogy(mhc_gains, 'b-', linewidth=2, label='mHC (Sinkhorn)', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Layer Depth', fontsize=12)\n",
    "plt.ylabel('Composite Forward Gain (log scale)', fontsize=12)\n",
    "plt.title('Signal Explosion: HC vs mHC Composite Mapping', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal forward gains at depth {depth}:\")\n",
    "print(f\"  Baseline: {baseline_gains[-1]:.2f}\")\n",
    "print(f\"  HC:       {hc_gains[-1]:.2e}  ← EXPLOSION!\")\n",
    "print(f\"  mHC:      {mhc_gains[-1]:.2f}  ← Stable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Money Plot: Recreating Figure 3\n",
    "\n",
    "This is the key visualization from the paper, showing why mHC matters for deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-quality plot for publication/sharing\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=150)\n",
    "\n",
    "# Re-run with fresh seeds\n",
    "depth = 64\n",
    "\n",
    "np.random.seed(42)\n",
    "baseline = simulate_depth(depth, 4, 'baseline')\n",
    "\n",
    "np.random.seed(42) \n",
    "hc = simulate_depth(depth, 4, 'hc')\n",
    "\n",
    "np.random.seed(42)\n",
    "mhc = simulate_depth(depth, 4, 'mhc', sinkhorn_iters=20)\n",
    "\n",
    "layers = range(1, depth + 1)\n",
    "\n",
    "ax.semilogy(layers, baseline, color='#10b981', linewidth=2.5, label='Baseline (Identity)', alpha=0.9)\n",
    "ax.semilogy(layers, hc, color='#ef4444', linewidth=2.5, label='HC (Unconstrained)', alpha=0.9)\n",
    "ax.semilogy(layers, mhc, color='#3b82f6', linewidth=2.5, label='mHC (Sinkhorn k=20)', alpha=0.9)\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Layer Depth', fontsize=13)\n",
    "ax.set_ylabel('Composite Forward Gain', fontsize=13)\n",
    "ax.set_title('The Manifold Dial: HC Explosion vs mHC Stability', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(1, depth)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate(f'HC: {hc[-1]:.1e}', xy=(depth, hc[-1]), xytext=(depth-15, hc[-1]*10),\n",
    "            fontsize=10, color='#ef4444',\n",
    "            arrowprops=dict(arrowstyle='->', color='#ef4444', alpha=0.7))\n",
    "\n",
    "ax.annotate(f'mHC: {mhc[-1]:.2f}', xy=(depth, mhc[-1]), xytext=(depth-15, mhc[-1]*0.1),\n",
    "            fontsize=10, color='#3b82f6',\n",
    "            arrowprops=dict(arrowstyle='->', color='#3b82f6', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mhc_hero_plot.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved as 'mhc_hero_plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Does This Work?\n",
    "\n",
    "### The Mathematics of Stability\n",
    "\n",
    "Doubly stochastic matrices have three key properties:\n",
    "\n",
    "1. **Spectral norm ≤ 1**: The matrix doesn't amplify signals\n",
    "2. **Closed under multiplication**: Products remain doubly stochastic  \n",
    "3. **Convex combinations of permutations**: Acts like a weighted average (Birkhoff-von Neumann)\n",
    "\n",
    "When you multiply many doubly stochastic matrices together, the result stays bounded because each multiplication is **non-expansive**.\n",
    "\n",
    "In contrast, unconstrained matrices compound their gains exponentially:\n",
    "- If each matrix has gain 1.1, after 64 layers: 1.1^64 ≈ 300\n",
    "- If each matrix has gain 1.5, after 64 layers: 1.5^64 ≈ 10^11 (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the mathematical property\n",
    "print(\"Demonstrating doubly stochastic closure under multiplication:\\n\")\n",
    "\n",
    "# Generate two doubly stochastic matrices\n",
    "A = sinkhorn_knopp(np.random.randn(4, 4), iterations=20)\n",
    "B = sinkhorn_knopp(np.random.randn(4, 4), iterations=20)\n",
    "\n",
    "# Their product\n",
    "C = A @ B\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(f\"  Row sums: {A.sum(axis=1).round(4)}\")\n",
    "print(f\"  Col sums: {A.sum(axis=0).round(4)}\")\n",
    "print(f\"  Spectral norm: {spectral_norm(A):.4f}\")\n",
    "\n",
    "print(\"\\nMatrix B:\")\n",
    "print(f\"  Row sums: {B.sum(axis=1).round(4)}\")\n",
    "print(f\"  Col sums: {B.sum(axis=0).round(4)}\")\n",
    "print(f\"  Spectral norm: {spectral_norm(B):.4f}\")\n",
    "\n",
    "print(\"\\nProduct C = A @ B:\")\n",
    "print(f\"  Row sums: {C.sum(axis=1).round(4)}\")\n",
    "print(f\"  Col sums: {C.sum(axis=0).round(4)}\")\n",
    "print(f\"  Spectral norm: {spectral_norm(C):.4f}\")\n",
    "print(\"\\n→ Product is also doubly stochastic! Spectral norm bounded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Manifold Dial: Interactive Experiment\n",
    "\n",
    "Let's see how varying Sinkhorn iterations affects the final composite gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep Sinkhorn iterations and see the effect\n",
    "sinkhorn_values = [0, 1, 2, 3, 5, 10, 15, 20, 25, 30]\n",
    "final_gains = []\n",
    "\n",
    "depth = 64\n",
    "n = 4\n",
    "\n",
    "for k in sinkhorn_values:\n",
    "    np.random.seed(42)  # Same seed for fair comparison\n",
    "    \n",
    "    if k == 0:\n",
    "        # k=0 means no projection, like HC\n",
    "        gains = simulate_depth(depth, n, 'hc')\n",
    "    else:\n",
    "        gains = simulate_depth(depth, n, 'mhc', sinkhorn_iters=k)\n",
    "    \n",
    "    final_gains.append(gains[-1])\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.semilogy(sinkhorn_values, final_gains, 'bo-', linewidth=2, markersize=8)\n",
    "ax.axhline(y=1, color='g', linestyle='--', alpha=0.5, label='Ideal (gain=1)')\n",
    "ax.axhline(y=2, color='orange', linestyle='--', alpha=0.5, label='Stable threshold')\n",
    "\n",
    "ax.set_xlabel('Sinkhorn Iterations (k)', fontsize=13)\n",
    "ax.set_ylabel('Final Composite Gain at Depth 64', fontsize=13)\n",
    "ax.set_title('The Manifold Dial: Stability vs Sinkhorn Iterations', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSinkhorn Iterations → Final Gain:\")\n",
    "for k, g in zip(sinkhorn_values, final_gains):\n",
    "    status = \"UNSTABLE\" if g > 10 else \"stable\" if g < 2 else \"marginal\"\n",
    "    print(f\"  k={k:2d}: {g:12.2e}  ({status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **HC (Hyper-Connections)** use unconstrained residual mixing matrices\n",
    "   - Each layer's matrix can have arbitrary row/column sums\n",
    "   - Over many layers, these compound → **exponential explosion**\n",
    "\n",
    "2. **mHC (Manifold-Constrained HC)** projects matrices onto the Birkhoff polytope\n",
    "   - Uses Sinkhorn-Knopp to ensure doubly stochastic matrices\n",
    "   - Spectral norm ≤ 1, so products stay bounded → **stable signals**\n",
    "\n",
    "3. **The \"Manifold Dial\"** is the Sinkhorn iteration count (k)\n",
    "   - k=0: Unconstrained (like HC) → unstable\n",
    "   - k≥10: Well-projected → stable\n",
    "   - Sweet spot around k=20 for most applications\n",
    "\n",
    "---\n",
    "\n",
    "**Try it yourself!** Modify the parameters above and re-run to explore:\n",
    "- Different depths (try 100, 200)\n",
    "- Different matrix sizes (n=2, 8, 16)\n",
    "- Different random seeds\n",
    "\n",
    "**Interactive Demo:** https://github.com/bassrehab/mhc-visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- **mHC Paper**: [DeepSeek-AI, arXiv:2512.24880](https://arxiv.org/abs/2512.24880)\n",
    "- **Sinkhorn-Knopp Algorithm**: Sinkhorn & Knopp (1967)\n",
    "- **This Notebook**: [github.com/bassrehab/mhc-visualizer](https://github.com/bassrehab/mhc-visualizer)\n",
    "\n",
    "**Author**: Subhadip Mitra (contact@subhadipmitra.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
